{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and Customize ICESat-2 Data Tutorial\n",
    "\n",
    "### This tutorial will walk you though how to access ICESat-2 data at the NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC) using spatial and temporal filters, as well as how to request customization services including subsetting and reformatting using an Application Programming Interface, or API. \n",
    "\n",
    "### Here is what you will learn in this tutorial:\n",
    "       1) Setting up NASA Earthdata Login authentication for direct data access.\n",
    "       2) Obtaining data set metadata.\n",
    "       3) Inputting data set search criteria using spatial and temporal filters. \n",
    "       4) Exploring different methods of specifying spatial criteria including lat/lon bounds, polygon coordinate pairs, and polygon input from a Shapefile or KML.  \n",
    "       5) Searching for matching files and receiving information about file volume.\n",
    "       6) Obtaining information about subsetting and reformatting capabilities for a specfic data set.\n",
    "       7) Configuring data request by \"chunking\" request by file number. \n",
    "       8) Submitting a request and monitoring the status of the request.\n",
    "       \n",
    "### Data Sets Used:\n",
    "        '/pine_island_glims/glims_polygons.kml'\n",
    "      \n",
    "### Remaining issues/ to do:\n",
    "    2) copy data from tutorials to S3 and add instructions to copy into user area\n",
    "       (if requests take too long then may the tutorial should just jump ahead and point to output\n",
    "        previously requested)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a token\n",
    "\n",
    "### We will generate a token needed in order to access data using your Earthdata Login credentials, and we will apply that token to the following queries. If you do not already have an Earthdata Login account, go to http://urs.earthdata.nasa.gov to register. Your password will be prompted for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Earthdata Login password:  ·········\n"
     ]
    }
   ],
   "source": [
    "# Earthdata Login credentials\n",
    "\n",
    "# Enter your Earthdata Login user name\n",
    "uid = 'amy.steiker'\n",
    "# Enter your email address associated with your Earthdata Login account\n",
    "email = 'amy.steiker@nsidc.org'\n",
    "pswd = getpass.getpass('Earthdata Login password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6596F866-438E-723D-9321-A8E2E4A4E099\n"
     ]
    }
   ],
   "source": [
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a data set of interest and determine the number and size of granules available within a time range and location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin discovering ICESat-2 data by first inputting the data set of interest.\n",
    "\n",
    "### See the [ICESat-2 Data Sets](https://nsidc.org/data/icesat-2/data-sets \"ICESat-2 Data Sets\") page for a list of all ICESat-2 data set titles and IDs. Below we will input data set ID ATL06, which is the ID for the \"ATLAS/ICESat-2 L3A Land Ice Height\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data set ID (e.g. ATL06) of interest here, also known as \"short name\".\n",
    "\n",
    "short_name = 'ATL06'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the ICESat-2 Data Sets page, you can find a link to each data set home page:\n",
    "\n",
    "ATL06: https://nsidc.org/data/atl06\n",
    "\n",
    "### From that home page, several resources are available, including an online user guide (within the User Guide tab of the landing page):\n",
    "https://nsidc.org/data/atl06?qt-data_set_tabs=3#qt-data_set_tabs\n",
    "\n",
    "### As well as a data dictionary with every data set variable described in detail:\n",
    "https://nsidc.org/sites/nsidc.org/files/technical-references/ATL06-data-dictionary-v001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will determine the most recent version number of our data set. We will also find out how many data granules (files) exist over an area and time of interest. [The Common Metadata Repository](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html \"CMR API documentation\") is queried to explore this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feed': {'entry': [{'archive_center': 'NASA NSIDC DAAC',\n",
      "                     'associations': {'services': ['S1568899363-NSIDC_ECS',\n",
      "                                                   'S1613689509-NSIDC_ECS',\n",
      "                                                   'S1613669681-NSIDC_ECS']},\n",
      "                     'boxes': ['-90 -180 90 180'],\n",
      "                     'browse_flag': False,\n",
      "                     'coordinate_system': 'CARTESIAN',\n",
      "                     'data_center': 'NSIDC_ECS',\n",
      "                     'dataset_id': 'ATLAS/ICESat-2 L3A Land Ice Height V001',\n",
      "                     'has_formats': True,\n",
      "                     'has_spatial_subsetting': True,\n",
      "                     'has_temporal_subsetting': True,\n",
      "                     'has_transforms': False,\n",
      "                     'has_variables': True,\n",
      "                     'id': 'C1511847675-NSIDC_ECS',\n",
      "                     'links': [{'href': 'https://n5eil01u.ecs.nsidc.org/ATLAS/ATL06.001/',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'http://nsidc.org/daac/subscriptions.html',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'https://search.earthdata.nasa.gov/search/granules?p=C1511847675-NSIDC_ECS&m=-87.87967837686685!9.890967019347585!1!1!0!0%2C2&tl=1542476530!4!!&q=atl06&ok=atl06',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'https://openaltimetry.org/',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'length': '0.0KB',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#'},\n",
      "                               {'href': 'https://doi.org/10.5067/ATLAS/ATL06.001',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/metadata#'},\n",
      "                               {'href': 'https://doi.org/10.5067/ATLAS/ATL06.001',\n",
      "                                'hreflang': 'en-US',\n",
      "                                'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#'}],\n",
      "                     'online_access_flag': True,\n",
      "                     'orbit_parameters': {'inclination_angle': '92.0',\n",
      "                                          'number_of_orbits': '0.071428571',\n",
      "                                          'period': '94.29',\n",
      "                                          'start_circular_latitude': '0.0',\n",
      "                                          'swath_width': '36.0'},\n",
      "                     'organizations': ['NASA NSIDC DAAC',\n",
      "                                       'NASA/GSFC/EOS/ESDIS'],\n",
      "                     'original_format': 'ISO19115',\n",
      "                     'processing_level_id': 'Level 3',\n",
      "                     'short_name': 'ATL06',\n",
      "                     'summary': 'This data set (ATL06) provides geolocated, '\n",
      "                                'land-ice surface heights (above the WGS 84 '\n",
      "                                'ellipsoid, ITRF2014 reference frame), plus '\n",
      "                                'ancillary parameters that can be used to '\n",
      "                                'interpret and assess the quality of the '\n",
      "                                'height estimates. The data were acquired by '\n",
      "                                'the Advanced Topographic Laser Altimeter '\n",
      "                                'System (ATLAS) instrument on board the Ice, '\n",
      "                                'Cloud and land Elevation Satellite-2 '\n",
      "                                '(ICESat-2) observatory.',\n",
      "                     'time_start': '2018-10-14T00:00:00.000Z',\n",
      "                     'title': 'ATLAS/ICESat-2 L3A Land Ice Height V001',\n",
      "                     'version_id': '001'}],\n",
      "          'id': 'https://cmr.earthdata.nasa.gov:443/search/collections.json?short_name=ATL06',\n",
      "          'title': 'ECHO dataset metadata',\n",
      "          'updated': '2019-06-09T15:26:03.554Z'}}\n"
     ]
    }
   ],
   "source": [
    "# Get json response from CMR collection metadata and print results. This provides high-level metadata on a data set or \"collection\", provide in json format.\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent version of  ATL06  is  001\n"
     ]
    }
   ],
   "source": [
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "#pprint.pprint(results['feed']['entry'])\n",
    "latest_version = max(versions)\n",
    "print('The most recent version of ', short_name, ' is ', latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the most recent version of this data set, let's determine the number of granules available over our area and time of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input temporal range \n",
    "\n",
    "# Input start date in yyyy-MM-dd format\n",
    "start_date = '2018-11-26'\n",
    "# Input start time in HH:mm:ss format\n",
    "start_time = '00:00:00'\n",
    "# Input end date in yyyy-MM-dd format\n",
    "end_date = '2018-11-27'\n",
    "# Input end time in HH:mm:ss format\n",
    "end_time = '23:59:59'\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three different options for inputting an area of interest to applied to our granule (file) search:\n",
    "#### 1) Bounding Box \n",
    "#### 2) Polygon coordinate pairs \n",
    "#### 3) Spatial file input, including Esri Shapefile or KML/KMZ. \n",
    "\n",
    "### If interested in the bounding box option, enter the lat/lon bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-55,67,-49,73\n"
     ]
    }
   ],
   "source": [
    "# Bounding Box spatial parameter in 'W,S,E,N' format\n",
    "\n",
    "# Input bounding box\n",
    "# Input lower left longitude in decimal degrees\n",
    "LL_lon = '-55'\n",
    "# Input lower left latitude in decimal degrees\n",
    "LL_lat = '67'\n",
    "# Input upper right longitude in decimal degrees\n",
    "UR_lon = '-49'\n",
    "# Input upper right latitude in decimal degrees\n",
    "UR_lat = '73'\n",
    "    \n",
    "bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "# aoi value used for subsetting logic below\n",
    "aoi = '1'\n",
    "print(bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If interested in the polygon coordinate pair option, enter the coordinate pairs. We can do this with separate x y lists that we can join and convert to the CMR parameter format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input lower left longitude in decimal degrees:  -120\n",
      "Input lower left latitude in decimal degrees:  70\n",
      "Input upper right longitude in decimal degrees:  -115\n",
      "Input upper right latitude in decimal degrees:  80\n"
     ]
    }
   ],
   "source": [
    "# Polygon coordinate pair spatial parameter\n",
    "\n",
    "#create list of x (longitude) values in decimal degrees\n",
    "x = [-86.622742, -86.561712, -86.868859, -86.850654, -86.717729, -86.737771, -86.602149, -86.622742]\n",
    "#create list of y (latitude) values in decimal degrees\n",
    "y = [-74.908126, -74.870913, -74.730522, -75.147247, -75.109052, -75.018662, -74.998483, -74.908126]\n",
    "xylist = list(zip(x, y))\n",
    "# Polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon. \n",
    "# Input polygon coordinates as comma separated values in longitude latitude order, i.e. lon1, lat1, lon2, lat2, lon3, lat3, and so on.\n",
    "polygon = ','.join(map(str, list(sum(xylist, ()))))\n",
    "print(polygon)\n",
    "# aoi value used for subsetting logic below\n",
    "aoi = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If interested in the Esri Shapefile or KML/KMZ input option, the following cell will:\n",
    "       1) Convert a file to geojson format,\n",
    "       2) Create a dictionary,\n",
    "       3) Simplify and reorder the polygon using the shapely package, and \n",
    "       4) Convert back to dictionary to be applied to the CMR polygon parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial file input, including Esri Shapefile or KML/KMZ\n",
    "    \n",
    "# POST shapefile or KML polygon to OGR for geojson conversion\n",
    "url = 'http://ogre.adc4gis.com/convert'\n",
    "shapefile = str(os.getcwd()) + '/pine_island_glims/glims_polygons.kml'\n",
    "files = {'upload': open(shapefile, 'rb')}\n",
    "r = requests.post(url, files=files)\n",
    "results = json.loads(r.content)\n",
    "    \n",
    "# Results is a dictionary representing a feature collection. List coordinates from the Polygon feature:\n",
    "polygon_list = list(results['features'][0]['geometry']['coordinates'][0])\n",
    "    \n",
    "    \n",
    "# Remove z value from polygon list\n",
    "for i in range(len(polygon_list)):\n",
    "    del polygon_list[i][2]\n",
    "    \n",
    "# Create shapely Polygon object for simplification and counter-clockwise ordering for CMR filtering\n",
    "poly = Polygon(tuple(polygon_list))\n",
    "    \n",
    "# Simplify polygon\n",
    "s = poly.simplify(0.05, preserve_topology=False)\n",
    "    \n",
    "# Orient counter-clockwise\n",
    "cc = orient(s, sign=1.0)\n",
    "    \n",
    "# From polygon object back to dictionary\n",
    "poly_dict = mapping(cc)\n",
    "\n",
    "# Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "# Polygon points need to be provided in counter-clockwise order as comma separated values in longitude latitude order, i.e. lon1, lat1, lon2, lat2, lon3, lat3, and so on. \n",
    "# The last point should match the first point to close the polygon. \n",
    "c = list(poly_dict['coordinates'][0])\n",
    "polygon = ','.join(map(str, list(sum(c, ()))))\n",
    "# aoi value used for subsetting logic below\n",
    "aoi = '3'\n",
    "print(polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the KML of Pine Island Glacier from the NSIDC [Global Land Ice Measurements from Space (GLIMS) database](http://www.glims.org/maps/glims). \n",
    "\n",
    "#### The Pine Island Glacier outline was downloaded from the following source: http://www.glims.org/maps/download.html?download_type=id&clause=G263560E76894S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-fe9db5946193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import geojson python dictionary as Geodataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#render figure in notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'features'"
     ]
    }
   ],
   "source": [
    "#import geojson python dictionary as Geodataframe\n",
    "gdf = gpd.GeoDataFrame.from_features(results['features'])\n",
    "\n",
    "#render figure in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "polygon_plot = gdf.plot(color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now populate dictionaries to be applied to our search query below based on spatial and temporal inputs. For additional search parameters, see the [The Common Metadata Repository API documentation](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html \"CMR API documentation\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMR search parameters:  {'short_name': 'ATL06', 'version': '001', 'temporal': '2018-11-26T00:00:00Z,2018-11-27T23:59:59Z', 'page_size': 100, 'page_num': 1, 'bounding_box': '-55,67,-49,73'}\n"
     ]
    }
   ],
   "source": [
    "#Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box\n",
    "    }\n",
    "else:\n",
    "# If polygon input (either via coordinate pairs or shapefile/KML/KMZ):\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'polygon': polygon,\n",
    "    }\n",
    "\n",
    "print('CMR search parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input the parameter dictionary to the CMR granule search to query all files (granules) that meet the criteria based on the granule metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 granules of ATL06 version 001 over my area and time of interest.\n"
     ]
    }
   ],
   "source": [
    "# Query number of granules using our (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', latest_version, 'over my area and time of interest.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now query the average size of those granules as well as the total volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average size of each granule is 76.66 MB and the total size of all 2 granules is 153.32 MB\n"
     ]
    }
   ],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although subsetting, reformatting, or reprojecting can alter the size of the granules, this \"native\" granule size can still be used to guide us towards the best download method to pursue, which we will come back to later on in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the subsetting and reformatting services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NSIDC DAAC supports customization services on many of our NASA Earthdata mission collections. Reformatting and subsetting are available on all Level-2 and -3 ICESat-2 data sets. Let's discover the specific service options supported for this data set and select which of these services we want to request. \n",
    "\n",
    "### We will start by querying the service capability endpoint and gather service information that we will select in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "#collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reprojection only applicable on ICESat-2 L3B products, yet to be available. \n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "#reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now select subsetting and reformatting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting by bounding box, based on the area of interest inputted above, is available.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to request this service? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting by time, based on the temporal range inputted above, is available.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to request this service? (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reformatting options are available: ['TABULAR_ASCII', 'NetCDF4-CF', 'Shapefile', 'NetCDF-3']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reprojection options are supported with your requested format\n"
     ]
    }
   ],
   "source": [
    "#print service information depending on service availability and select service options\n",
    "    \n",
    "if len(subagent) < 1 :\n",
    "    print('No services exist for', short_name, 'version', latest_version)\n",
    "    agent = 'NO'\n",
    "    bbox = ''\n",
    "    timevar = ''\n",
    "    reformat = ''\n",
    "    projection = ''\n",
    "    projection_parameters = ''\n",
    "    coverage = ''\n",
    "    polygon = ''\n",
    "else:\n",
    "    agent = ''\n",
    "    subdict = subagent[0]\n",
    "    #icesat-2 spatial subsetting logic\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '1':\n",
    "        print('Subsetting by bounding box, based on the area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bounding_shape = ''\n",
    "        polygon = ''\n",
    "        if ss == 'y': \n",
    "            bbox = bounding_box\n",
    "        else: \n",
    "            bbox = ''\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '2':\n",
    "        print('Subsetting by polygon, based on area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bounding_box = ''\n",
    "        bounding_shape = ''\n",
    "        if ss == 'y':\n",
    "            #get polygon bounds to be used as bounding box input\n",
    "            #Create shapely Polygon object from x y list\n",
    "            p = Polygon(tuple(xylist))\n",
    "            # Extract the point values that define the perimeter of the polygon\n",
    "            bounds = p.bounds\n",
    "            bbox = ','.join(map(str, list(bounds)))\n",
    "        else: \n",
    "            bbox = ''\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '3':\n",
    "        print('Subsetting by polygon, based on the original spatial file inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bbox = ''\n",
    "        bounding_box = ''\n",
    "        bounding_shape = ''\n",
    "        if ss == 'y': upload = 'T'\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] != 'ICESAT2':\n",
    "        polygon = ''\n",
    "        print('Subsetting by bounding box, based on the area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        if ss == 'y': bbox = bounding_box\n",
    "        else: \n",
    "            bbox = ''\n",
    "            polygon = ''\n",
    "    if subdict['temporalSubsetting'] == 'true':\n",
    "        print('Subsetting by time, based on the temporal range inputted above, is available.')\n",
    "        ts = input('Would you like to request this service? (y/n)')\n",
    "        if ts == 'y': timevar = temporal \n",
    "        else: timevar = ''\n",
    "    else: timevar = ''\n",
    "    if len(format_vals) > 0 :\n",
    "        print('These reformatting options are available:', format_vals)\n",
    "        reformat = input('If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank.')\n",
    "        # select reprojection options based on reformatting selection\n",
    "        if reformat in format_proj and len(proj_vals) > 0 : \n",
    "            print('These reprojection options are available with your requested format:', proj_vals)\n",
    "            projection = input('If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes, e.g. GEOGRAPHIC), otherwise leave blank.')\n",
    "            # Enter required parameters for UTM North and South\n",
    "            if projection == 'UTM NORTHERN HEMISPHERE' or projection == 'UTM SOUTHERN HEMISPHERE': \n",
    "                NZone = input('Please enter a UTM zone (1 to 60 for Northern Hemisphere; -60 to -1 for Southern Hemisphere):')\n",
    "                projection_parameters = str('NZone:' + NZone)\n",
    "            else: projection_parameters = ''\n",
    "        else: \n",
    "            print('No reprojection options are supported with your requested format')\n",
    "            projection = ''\n",
    "            projection_parameters = ''\n",
    "    else: \n",
    "        reformat = ''\n",
    "        projection = ''\n",
    "        projection_parameters = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because variable subsetting can include a long list of variables to choose from, we will decide on variable subsetting separately from the service options above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Variable subsetting is available. Would you like to subset a selection of variables? (y/n) n\n"
     ]
    }
   ],
   "source": [
    "# Select variable subsetting\n",
    "\n",
    "if len(variable_vals) > 0:\n",
    "        v = input('Variable subsetting is available. Would you like to subset a selection of variables? (y/n)')\n",
    "        if v == 'y':\n",
    "            print('The', short_name, 'variables to select from include:')\n",
    "            pprint.pprint(variable_vals)\n",
    "            coverage = input('If you would like to subset by variable, copy and paste the variables you would like separated by comma. Make sure to omit quotes but include all forward slashes: ')\n",
    "        else: coverage = ''\n",
    "\n",
    "#no services selected\n",
    "if reformat == '' and projection == '' and projection_parameters == '' and coverage == '' and timevar == '' and bbox == '' and aoi != '3':\n",
    "    agent = 'NO'\n",
    "else: agent = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request data from the NSIDC data access API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _What is an API? API stands for Application Programming Interface. You can think of it as a middle man between an application or end-use (in this case, us) and a data provider (in this case, the Common Metadata Repository and NSIDC). These APIs are essentially structured as a URL with a base plus individual key-value-pairs (KVPs) separated by ‘&’._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now set up our data download request. Recall that we queried the total number and volume of granules prior to applying customization services, so you can use these values to adjust the number of granules per request up to a limit of 100 granules. \n",
    "\n",
    "### For now, let's select 10 granules to be processed in each zipped request. For ATL06, the granule size can exceed 100 MB so we want to choose a granule count that provides us with a reasonable zipped download size. We will also set the request mode to asynchronous, which will allow concurrent requests to be queued and processed without the need for a continuous connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 1 total order(s) processed for our ATL06 request.\n"
     ]
    }
   ],
   "source": [
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set request mode. Request mode is \"synchronous\" by default, meaning that the request relies on a direct, continous connection between you and the API endpoint. Outputs are directly downloaded, or \"streamed\" to your working directory.\n",
    "request_mode = 'async'\n",
    "\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "print('There will be', page_num, 'total order(s) processed for our', short_name, 'request.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://n5eil02u.ecs.nsidc.org/egi/request?short_name=ATL06&version=001&temporal=2018-11-26T00:00:00Z,2018-11-27T23:59:59Z&time=&bounding_box=-55,67,-49,73&polygon=&bbox=-55,67,-49,73&bounding_shape=&format=&projection=&projection_parameters=&Coverage=&request_mode=async&page_size=10&page_num=1&agent=&token=6596F866-438E-723D-9321-A8E2E4A4E099&email=amy.steiker@nsidc.org\n"
     ]
    }
   ],
   "source": [
    "#Print API base URL + request parameters\n",
    "API_request = base_url + '?' + 'short_name=' + short_name + '&' + 'version=' + latest_version + '&' + 'temporal=' + temporal + '&' + 'time=' + timevar + '&' + 'bounding_box=' + bounding_box + '&' + 'polygon=' + polygon + '&' + 'bbox=' + bbox + '&' + 'bounding_shape=' + bounding_shape + '&' +  'format=' + reformat + '&' + 'projection=' + projection + '&' + 'projection_parameters=' + projection_parameters + '&' + 'Coverage=' + coverage + '&' + 'request_mode=' + request_mode + '&' + 'page_size=' + str(page_size) + '&' + 'page_num=1&' + 'agent=' + agent + '&' + 'token=' + token + '&' + 'email=' + email\n",
    "print(API_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's download the data directly to this notebook directory in a new Outputs folder. The progress of each order will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'short_name': 'ATL06', 'version': '001', 'temporal': '2018-11-26T00:00:00Z,2018-11-27T23:59:59Z', 'time': '', 'bounding_box': '-55,67,-49,73', 'polygon': '', 'bbox': '-55,67,-49,73', 'bounding_shape': '', 'format': '', 'projection': '', 'projection_parameters': '', 'Coverage': '', 'request_mode': 'async', 'page_size': 10, 'page_num': 1, 'agent': '', 'token': '6596F866-438E-723D-9321-A8E2E4A4E099', 'email': 'amy.steiker@nsidc.org'}\n",
      "order ID 5000000317532\n",
      "status URL: https://n5eil02u.ecs.nsidc.org/egi/request/5000000317532\n",
      "HTTP request response: \n",
      "Data request 1 is submitting...\n",
      "Initial request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is complete\n",
      "Request status is complete. Beginning download of zipped output.\n",
      "Data request 1 is complete.\n"
     ]
    }
   ],
   "source": [
    "# Create Outputs folder if folder does not already exist, request data service for each page number, and unzip outputs\n",
    "\n",
    "path = str(os.getcwd() + '/Outputs')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print(page_val)\n",
    "    request_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'time': timevar, 'bounding_box': bounding_box, 'polygon': polygon, 'bbox': bbox, 'bounding_shape': bounding_shape, 'format': reformat, 'projection': projection, 'projection_parameters': projection_parameters, 'Coverage': coverage, 'request_mode': request_mode, 'page_size': page_size, 'page_num': page_val, 'agent': agent, 'token': token, 'email': email, }\n",
    "    print(request_params)\n",
    "    #Initial order response \n",
    "    data_session = requests.session()\n",
    "    data_s = data_session.get(base_url, params=request_params)\n",
    "    data_response = session.get(data_s.url,auth=(uid,pswd))\n",
    "    esir_root = ET.fromstring(data_response.content)\n",
    "    #Look up order ID\n",
    "    orderlist = []   \n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID' + ' ' + orderID)\n",
    "\n",
    "    #Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ' + statusURL)\n",
    "    #Find order status\n",
    "    request_session = requests.session()\n",
    "    request_s = request_session.get(statusURL)\n",
    "    request_response = session.get(request_s.url,auth=(uid,pswd))\n",
    "    print('HTTP request response: '), request_response\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request', page_val, 'is submitting...')\n",
    "    print('Initial request status is ' + status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        time.sleep(5)\n",
    "        loop_session = requests.session()\n",
    "        loop_s = loop_session.get(statusURL)\n",
    "        loop_response = session.get(loop_s.url,auth=(uid,pswd))\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "        #find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is' + ' ' + status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "\n",
    "    print('Request status is complete. Beginning download of zipped output.')\n",
    "    downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "    zip_session = requests.session()\n",
    "    zip_s = zip_session.get(downloadURL)\n",
    "    zip_response = session.get(zip_s.url,auth=(uid,pswd))\n",
    "    with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "        z.extractall(path)\n",
    "    print('Data request', page_val, 'is complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_ATL06_20181126163030_09020103_001_01.h5',\n",
       " 'processed_ATL06_20181127051524_09100105_001_01.h5']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List files\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data-access/notebooks/Outputs\n"
     ]
    }
   ],
   "source": [
    "#remove Outputs folder for testing\n",
    "\n",
    "#shutil.rmtree(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, and downloaded data directly to our local machine. You are welcome to request different data sets, areas of interest, and/or customization services by re-running the notebook or starting again at the 'Select a data set of interest' step above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
