{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and Customize ICESat-2 Data Tutorial\n",
    "\n",
    "### This tutorial will walk you though how to access ICESat-2 data at the NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC) using spatial and temporal filters, as well as how to request customization services including subsetting and reformatting using an Application Programming Interface, or API. \n",
    "\n",
    "### Here is what you will learn in this tutorial:\n",
    "       1) Setting up NASA Earthdata Login authentication for direct data access.\n",
    "       2) Obtaining data set metadata.\n",
    "       3) Inputting data set search criteria using spatial and temporal filters. \n",
    "       4) Exploring different methods of specifying spatial criteria including lat/lon bounds, polygon coordinate pairs, and polygon input from a Shapefile or KML.  \n",
    "       5) Searching for matching files and receiving information about file volume.\n",
    "       6) Obtaining information about subsetting and reformatting capabilities for a specfic data set.\n",
    "       7) Configuring data request by \"chunking\" request by file number. \n",
    "       8) Submitting a request and monitoring the status of the request.\n",
    "       \n",
    "### Data Sets Used:\n",
    "`s3://pangeo-data-upload-oregon/icesat2/pine_island_glims/glims_polygons.kml`\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import fiona\n",
    "# To read KML files with geopandas, we will need to enable KML support in fiona (disabled by default)\n",
    "fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from s3 bucket\n",
    "\n",
    "### We will first pull a Pine Island Glacier outline from our Amazon s3 bucket, which we will use below for filtering and subsetting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to pull pine island glacier folder from s3:\n",
    "\n",
    "    1) Open terminal window in Pangeo: File -> New -> Terminal\n",
    "    2) Move to the data-access notebook working directory by changing to the following directory:\n",
    "\n",
    "`cd /home/jovyan/data-access/notebooks`\n",
    "    \n",
    "    3) Copy the following line to terminal and hit enter:\n",
    "\n",
    "`aws s3 sync s3://pangeo-data-upload-oregon/icesat2/pine_island_glims/ pine_island_glims/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pine Island Glacier outline was originally downloaded from the the NSIDC [Global Land Ice Measurements from Space (GLIMS) database](http://www.glims.org/maps/glims). Direct download access: http://www.glims.org/maps/info.html?anlys_id=528486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a token\n",
    "\n",
    "### We will generate a token needed in order to access data using your Earthdata Login credentials, and we will apply that token to the following queries. If you do not already have an Earthdata Login account, go to http://urs.earthdata.nasa.gov to register. Your password will be prompted for privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earthdata Login credentials\n",
    "\n",
    "# Enter your Earthdata Login user name\n",
    "uid = 'amy.steiker'\n",
    "# Enter your email address associated with your Earthdata Login account\n",
    "email = 'amy.steiker@nsidc.org'\n",
    "pswd = getpass.getpass('Earthdata Login password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a data set of interest and determine the number and size of granules available within a time range and location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin discovering ICESat-2 data by first inputting the data set of interest.\n",
    "\n",
    "### See the [ICESat-2 Data Sets](https://nsidc.org/data/icesat-2/data-sets \"ICESat-2 Data Sets\") page for a list of all ICESat-2 data set titles and IDs. Below we will input data set ID ATL06, which is the ID for the \"ATLAS/ICESat-2 L3A Land Ice Height\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data set ID (e.g. ATL06) of interest here, also known as \"short name\".\n",
    "\n",
    "short_name = 'ATL06'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the ICESat-2 Data Sets page, you can find a link to each data set home page:\n",
    "\n",
    "ATL06: https://nsidc.org/data/atl06\n",
    "\n",
    "### From that home page, several resources are available, including an online user guide (within the User Guide tab of the landing page):\n",
    "https://nsidc.org/data/atl06?qt-data_set_tabs=3#qt-data_set_tabs\n",
    "\n",
    "### As well as a data dictionary with every data set variable described in detail:\n",
    "https://nsidc.org/sites/nsidc.org/files/technical-references/ATL06-data-dictionary-v001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will determine the most recent version number of our data set. We will also find out how many data granules (files) exist over an area and time of interest. [The Common Metadata Repository](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html \"CMR API documentation\") is queried to explore this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get json response from CMR collection metadata and print results. This provides high-level metadata on a data set or \"collection\", provide in json format.\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print('The most recent version of ', short_name, ' is ', latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the most recent version of this data set, let's determine the number of granules available over our area and time of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input temporal range \n",
    "\n",
    "# Input start date in yyyy-MM-dd format\n",
    "start_date = '2019-02-22'\n",
    "# Input start time in HH:mm:ss format\n",
    "start_time = '00:00:00'\n",
    "# Input end date in yyyy-MM-dd format\n",
    "end_date = '2019-02-22'\n",
    "# Input end time in HH:mm:ss format\n",
    "end_time = '23:59:59'\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three different options for inputting an area of interest to applied to our granule (file) search:\n",
    "#### 1) Bounding Box \n",
    "#### 2) Polygon coordinate pairs \n",
    "#### 3) Spatial file input, including Esri Shapefile or KML/KMZ. \n",
    "\n",
    "### If interested in the bounding box option, enter the lat/lon bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commenting for tutorial since we will be walking through option 3 (spatial file input) together\n",
    "# # Bounding Box spatial parameter in 'W,S,E,N' format\n",
    "\n",
    "# # Input bounding box\n",
    "# # Input lower left longitude in decimal degrees\n",
    "# LL_lon = ''\n",
    "# # Input lower left latitude in decimal degrees\n",
    "# LL_lat = ''\n",
    "# # Input upper right longitude in decimal degrees\n",
    "# UR_lon = ''\n",
    "# # Input upper right latitude in decimal degrees\n",
    "# UR_lat = ''\n",
    "\n",
    "# bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "# # aoi value used for subsetting logic below\n",
    "# aoi = '1'\n",
    "# print(bounding_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If interested in the polygon coordinate pair option, enter the coordinate pairs. We can do this with separate x y lists that we can join and convert to the CMR parameter format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commenting for tutorial since we will be walking through option 3 (spatial file input) together\n",
    "# # Polygon coordinate pair spatial parameter\n",
    "\n",
    "# #create list of x (longitude) values in decimal degrees\n",
    "# x = []\n",
    "# #create list of y (latitude) values in decimal degrees\n",
    "# y = []\n",
    "# xylist = list(zip(x, y))\n",
    "# # Polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon. \n",
    "# # Input polygon coordinates as comma separated values in longitude latitude order, i.e. lon1, lat1, lon2, lat2, lon3, lat3, and so on.\n",
    "# polygon = ','.join(map(str, list(sum(xylist, ()))))\n",
    "# print(polygon)\n",
    "# # aoi value used for subsetting logic below\n",
    "# aoi = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If interested in the Esri Shapefile or KML/KMZ input option, the following cell will:\n",
    "       1) Convert a file to geojson format,\n",
    "       2) Create a dictionary,\n",
    "       3) Simplify and reorder the polygon using the shapely package, and \n",
    "       4) Convert back to dictionary to be applied to the CMR polygon parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial file input, including Esri Shapefile or KML/KMZ\n",
    "    \n",
    "## POST shapefile or KML polygon to OGR for geojson conversion\n",
    "#url = 'http://ogre.adc4gis.com/convert'\n",
    "#shapefile = str(os.getcwd()) + '/pine_island_glims/glims_polygons.kml'\n",
    "#files = {'upload': open(shapefile, 'rb')}\n",
    "#r = requests.post(url, files=files)\n",
    "#results = json.loads(r.content)\n",
    "## Results is a dictionary representing a feature collection. List coordinates from the Polygon feature:\n",
    "#polygon_list = list(results['features'][0]['geometry']['coordinates'][0])     \n",
    "## Remove z value from polygon list\n",
    "#for i in range(len(polygon_list)):\n",
    "#    del polygon_list[i][2] \n",
    "## Create shapely Polygon object for simplification and counter-clockwise ordering for CMR filtering\n",
    "#poly = Polygon(tuple(polygon_list))\n",
    "\n",
    "\n",
    "# Use geopandas to read in polygon file\n",
    "# Note: a shapefile could be substituted here, or geojson, or...\n",
    "kml_filepath = str(os.getcwd()) + '/pine_island_glims/glims_polygons.kml'\n",
    "\n",
    "gdf = gpd.read_file(kml_filepath)\n",
    "poly = gdf.iloc[0].geometry\n",
    "\n",
    "#simplify polygon\n",
    "poly = poly.simplify(0.05, preserve_topology=False)\n",
    "\n",
    "# Orient counter-clockwise\n",
    "poly = orient(poly, sign=1.0)\n",
    "\n",
    "#Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "# Polygon points need to be provided in counter-clockwise order as comma separated values in longitude latitude order, i.e. lon1, lat1, lon2, lat2, lon3, lat3, and so on. \n",
    "# The last point should match the first point to close the polygon. \n",
    "polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "\n",
    "# aoi value used for subsetting logic below\n",
    "aoi = '3'\n",
    "print(polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the KML of Pine Island Glacier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#render figure in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "polygon_plot = gdf.plot(color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now populate dictionaries to be applied to our search query below based on spatial and temporal inputs. For additional search parameters, see the [The Common Metadata Repository API documentation](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html \"CMR API documentation\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CMR parameters used for granule search. Modify params depending on bounding_box or polygon input.\n",
    "\n",
    "if aoi == '1':\n",
    "# bounding box input:\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'bounding_box': bounding_box\n",
    "    }\n",
    "else:\n",
    "# If polygon input (either via coordinate pairs or shapefile/KML/KMZ):\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1,\n",
    "    'polygon': polygon,\n",
    "    }\n",
    "\n",
    "print('CMR search parameters: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input the parameter dictionary to the CMR granule search to query all files (granules) that meet the criteria based on the granule metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query number of granules using our (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', latest_version, 'over my area and time of interest.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can view this in the [NASA Earthdata Search web interface](https://search.earthdata.nasa.gov/search/granules?polygon=-86.625%2C-74.900390625%2C-87.029296875%2C-74.6015625%2C-90.298828125%2C-74.021484375%2C-93.427734375%2C-73.93359375%2C-94.359375%2C-73.74023437500001%2C-96.767578125%2C-74.126953125%2C-100.107421875%2C-74.021484375%2C-100.828125%2C-74.37304687500001%2C-102.427734375%2C-74.49609375%2C-101.25%2C-74.70703125%2C-101.548828125%2C-75.02343750000001%2C-104.009765625%2C-75.515625%2C-102.357421875%2C-75.744140625%2C-101.28515625%2C-76.201171875%2C-101.197265625%2C-76.271484375%2C-101.443359375%2C-76.658203125%2C-101.03906250000001%2C-76.93945312500001%2C-96.521484375%2C-77.484375%2C-96.43359375%2C-77.677734375%2C-97.611328125%2C-78.029296875%2C-95.02734375%2C-78.591796875%2C-94.9921875%2C-78.732421875%2C-95.677734375%2C-78.99609375%2C-95.27343750000001%2C-79.119140625%2C-95.431640625%2C-79.2421875%2C-93.990234375%2C-79.611328125%2C-93.884765625%2C-79.875%2C-93.234375%2C-80.0859375%2C-91.705078125%2C-79.875%2C-91.810546875%2C-79.857421875%2C-91.494140625%2C-79.8046875%2C-91.458984375%2C-79.646484375%2C-90.43945312500001%2C-79.59375%2C-90.544921875%2C-79.55859375%2C-90.03515625%2C-79.2421875%2C-88.98046875%2C-79.083984375%2C-92.03906250000001%2C-78.416015625%2C-92.109375%2C-78.310546875%2C-90.73828125%2C-77.90625000000001%2C-92.390625%2C-77.501953125%2C-92.197265625%2C-77.37890625%2C-92.337890625%2C-77.203125%2C-91.01953125%2C-77.150390625%2C-91.880859375%2C-76.869140625%2C-87.064453125%2C-75.884765625%2C-86.87109375%2C-75.708984375%2C-87.08203125%2C-75.4453125%2C-86.607421875%2C-75.005859375%2C-86.625%2C-74.900390625&p=C1511847675-NSIDC_ECS!C1511847675-NSIDC_ECS&pg[1][v]=t&m=-74.09615279797836!-130.36684058200473!1!2!0!0%2C2&qt=2019-02-22T00%3A00%3A00.000Z%2C2019-02-22T23%3A59%3A59.000Z&q=atl06&ok=atl06&sf=5633090487), which relies on the same metadata, although their simplified polygon may differ slightly. With the same search criteria applied, we can view the same 16 granules of ATL06 over the glacier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now query the average size of those granules as well as the total volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Although subsetting, reformatting, or reprojecting can alter the size of the granules, this \"native\" granule size can still be used to guide us towards the best download method to pursue, which we will come back to later on in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the subsetting and reformatting services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NSIDC DAAC supports customization services on many of our NASA Earthdata mission collections. Reformatting and subsetting are available on all Level-2 and -3 ICESat-2 data sets. Let's discover the specific service options supported for this data set and select which of these services we want to request. \n",
    "\n",
    "### We will start by querying the service capability endpoint and gather service information that we will select in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "#collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reprojection only applicable on ICESat-2 L3B products, yet to be available. \n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "#reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now select subsetting and reformatting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print service information depending on service availability and select service options\n",
    "    \n",
    "if len(subagent) < 1 :\n",
    "    print('No services exist for', short_name, 'version', latest_version)\n",
    "    agent = 'NO'\n",
    "    bbox = ''\n",
    "    timevar = ''\n",
    "    reformat = ''\n",
    "    projection = ''\n",
    "    projection_parameters = ''\n",
    "    coverage = ''\n",
    "    polygon = ''\n",
    "else:\n",
    "    agent = ''\n",
    "    subdict = subagent[0]\n",
    "    #icesat-2 spatial subsetting logic\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '1':\n",
    "        print('Subsetting by bounding box, based on the area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bounding_shape = ''\n",
    "        polygon = ''\n",
    "        if ss == 'y': \n",
    "            bbox = bounding_box\n",
    "        else: \n",
    "            bbox = ''\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '2':\n",
    "        print('Subsetting by polygon, based on area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bounding_box = ''\n",
    "        bounding_shape = ''\n",
    "        if ss == 'y':\n",
    "            #get polygon bounds to be used as bounding box input\n",
    "            #Create shapely Polygon object from x y list\n",
    "            p = Polygon(tuple(xylist))\n",
    "            # Extract the point values that define the perimeter of the polygon\n",
    "            bounds = p.bounds\n",
    "            bbox = ','.join(map(str, list(bounds)))\n",
    "        else: \n",
    "            bbox = ''\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '3':\n",
    "        print('Subsetting by polygon, based on the original spatial file inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bbox = ''\n",
    "        bounding_box = ''\n",
    "        bounding_shape = ''\n",
    "        if ss == 'y': upload = 'T'\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] != 'ICESAT2':\n",
    "        polygon = ''\n",
    "        print('Subsetting by bounding box, based on the area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        if ss == 'y': bbox = bounding_box\n",
    "        else: \n",
    "            bbox = ''\n",
    "            polygon = ''\n",
    "    if subdict['temporalSubsetting'] == 'true':\n",
    "        print('Subsetting by time, based on the temporal range inputted above, is available.')\n",
    "        ts = input('Would you like to request this service? (y/n)')\n",
    "        if ts == 'y': timevar = temporal \n",
    "        else: timevar = ''\n",
    "    else: timevar = ''\n",
    "    if len(format_vals) > 0 :\n",
    "        print('These reformatting options are available:', format_vals)\n",
    "        reformat = input('If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank.')\n",
    "        # select reprojection options based on reformatting selection\n",
    "        if reformat in format_proj and len(proj_vals) > 0 : \n",
    "            print('These reprojection options are available with your requested format:', proj_vals)\n",
    "            projection = input('If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes, e.g. GEOGRAPHIC), otherwise leave blank.')\n",
    "            # Enter required parameters for UTM North and South\n",
    "            if projection == 'UTM NORTHERN HEMISPHERE' or projection == 'UTM SOUTHERN HEMISPHERE': \n",
    "                NZone = input('Please enter a UTM zone (1 to 60 for Northern Hemisphere; -60 to -1 for Southern Hemisphere):')\n",
    "                projection_parameters = str('NZone:' + NZone)\n",
    "            else: projection_parameters = ''\n",
    "        else: \n",
    "            print('No reprojection options are supported with your requested format')\n",
    "            projection = ''\n",
    "            projection_parameters = ''\n",
    "    else: \n",
    "        reformat = ''\n",
    "        projection = ''\n",
    "        projection_parameters = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because variable subsetting can include a long list of variables to choose from, we will decide on variable subsetting separately from the service options above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variable subsetting\n",
    "\n",
    "if len(variable_vals) > 0:\n",
    "        v = input('Variable subsetting is available. Would you like to subset a selection of variables? (y/n)')\n",
    "        if v == 'y':\n",
    "            print('The', short_name, 'variables to select from include:')\n",
    "            pprint.pprint(variable_vals)\n",
    "            coverage = input('If you would like to subset by variable, copy and paste the variables you would like separated by comma. Make sure to omit quotes but include all forward slashes: ')\n",
    "        else: coverage = ''\n",
    "\n",
    "#If no services selected, set \"agent\" to NO, which will tell the endpoint not to send the request through to the service processor. Instead, it will quickly send back the data \"natively\" without any customization applied.\n",
    "if reformat == '' and projection == '' and projection_parameters == '' and coverage == '' and timevar == '' and bbox == '' and bounding_shape == '' and aoi != '3':\n",
    "    agent = 'NO'\n",
    "elif aoi == '3' and ss != 'y':\n",
    "    agent = 'NO'\n",
    "else: agent = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request data from the NSIDC data access API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _What is an API? API stands for Application Programming Interface. You can think of it as a middle man between an application or end-use (in this case, us) and a data provider (in this case, the Common Metadata Repository and NSIDC). These APIs are essentially structured as a URL with a base plus individual key-value-pairs (KVPs) separated by ‘&’._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now set up our data download request. Recall that we queried the total number and volume of granules prior to applying customization services, so you can use these values to adjust the number of granules per request up to a limit of 100 granules. \n",
    "\n",
    "### For now, let's select 10 granules to be processed in each zipped request. For ATL06, the granule size can exceed 100 MB so we want to choose a granule count that provides us with a reasonable zipped download size. We will also set the request mode to asynchronous, which will allow concurrent requests to be queued and processed without the need for a continuous connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set request mode. Request mode is \"synchronous\" by default, meaning that the request relies on a direct, continous connection between you and the API endpoint. Outputs are directly downloaded, or \"streamed\" to your working directory.\n",
    "request_mode = 'async'\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "print('There will be', page_num, 'total order(s) processed for our', short_name, 'request.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print API base URL + request parameters\n",
    "API_request = base_url + '?' + 'short_name=' + short_name + '&' + 'version=' + latest_version + '&' + 'temporal=' + temporal + '&' + 'time=' + timevar + '&' + 'bounding_box=' + bounding_box + '&' + 'polygon=' + polygon + '&' + 'bbox=' + bbox + '&' + 'bounding_shape=' + bounding_shape + '&' +  'format=' + reformat + '&' + 'projection=' + projection + '&' + 'projection_parameters=' + projection_parameters + '&' + 'Coverage=' + coverage + '&' + 'request_mode=' + request_mode + '&' + 'page_size=' + str(page_size) + '&' + 'page_num=1&' + 'agent=' + agent + '&' + 'token=' + token + '&' + 'email=' + email\n",
    "print(API_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's download the data directly to this notebook directory in a new Outputs folder. The progress of each order will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Outputs folder if folder does not already exist\n",
    "\n",
    "path = str(os.getcwd() + '/Outputs')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "# Request data service for each page number, and unzip outputs\n",
    "\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    request_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'time': timevar, 'bounding_box': bounding_box, 'polygon': polygon, 'bbox': bbox, 'bounding_shape': bounding_shape, 'format': reformat, 'projection': projection, 'projection_parameters': projection_parameters, 'Coverage': coverage, 'request_mode': request_mode, 'page_size': page_size, 'page_num': page_val, 'agent': agent, 'token': token, 'email': email, }\n",
    "    # Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "    if aoi == '3' and ss == 'y':\n",
    "        shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "        request = session.post(base_url, params=request_params, files=shape_post) \n",
    "    else:\n",
    "        request = session.get(base_url, params=request_params)\n",
    "    print('Request HTTP response: ', request.status_code)\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "    print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "    print('Order request response XML content: ', request.content)\n",
    "    #Look up order ID\n",
    "    orderlist = []   \n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "    #Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "    #Find order status\n",
    "    request_response = session.get(statusURL)    \n",
    "    print('HTTP response from order response URL: ', request_response.status_code)\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        time.sleep(5)\n",
    "        loop_response = session.get(statusURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "        #find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "    #Order can either complete, complete_with_errors, or fail:\n",
    "    # Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        message = messagelist[0]\n",
    "        print('complete_with_error message: ', message)\n",
    "    # Download zipped order if status is complete or complete_with_errors\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        print('Beginning download of zipped output...')\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Zip download URL: ', downloadURL)\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List files\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Outputs folder for testing\n",
    "#path = str(os.getcwd() + '/Outputs')\n",
    "#shutil.rmtree(path)\n",
    "\n",
    "glacier_path = str(os.getcwd() + '/pine_island_glims')\n",
    "#print(glacier_path)\n",
    "shutil.rmtree(glacier_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, and downloaded data directly to our local machine. You are welcome to request different data sets, areas of interest, and/or customization services by re-running the notebook or starting again at the 'Select a data set of interest' step above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
