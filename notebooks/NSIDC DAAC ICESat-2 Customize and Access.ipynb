{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSIDC DAAC ICESat-2 Customize and Access Data Tutorial\n",
    "\n",
    "### This tutorial will walk you though how to access ICESat-2 data at the NASA National Snow and Ice Data Center Distributed Active Archive Center (NSIDC DAAC) using spatial and temporal filters, as well as how to request customization services including subsetting and reformatting using an API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import pprint\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a token\n",
    "\n",
    "### We will generate a token needed in order to access data using your Earthdata Login credentials, and we will apply that token to the following queries. If you do not already have an Earthdata Login account, go to http://urs.earthdata.nasa.gov to register. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Earthdata Login user name:  amy.steiker\n",
      "Earthdata Login password:  ·········\n",
      "Email address associated with Earthdata Login account:  amy.steiker@nsidc.org\n"
     ]
    }
   ],
   "source": [
    "# Earthdata Login credentials\n",
    "\n",
    "uid = input('Earthdata Login user name: ')\n",
    "pswd = getpass.getpass('Earthdata Login password: ')\n",
    "email = input('Email address associated with Earthdata Login account: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67F8A83F-CB95-D836-45CF-30638D6DE8AD\n"
     ]
    }
   ],
   "source": [
    "# Request token from Common Metadata Repository using Earthdata credentials\n",
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "hostname = socket.gethostname()\n",
    "ip = socket.gethostbyname(hostname)\n",
    "\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a data set of interest and determine the number and size of granules available within a time range and location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin discovering ICESat-2 data by first inputting the data set of interest and determining the most recent version number. We will also find out how many data granules exist over an area and time of interest. The Common Metadata Repository is queried to explore this information. See https://nsidc.org/data/icesat-2/data-sets for a list of all ICESat-2 data set titles and short names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input short name, e.g. ATL06, here:  ATL06\n"
     ]
    }
   ],
   "source": [
    "# Input data set short name (e.g. ATL06) of interest here.\n",
    "\n",
    "short_name = input('Input short name, e.g. ATL06, here: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent version of  ATL06  is  001\n"
     ]
    }
   ],
   "source": [
    "# Get json response from CMR collection metadata\n",
    "\n",
    "params = {\n",
    "    'short_name': short_name\n",
    "}\n",
    "\n",
    "cmr_collections_url = 'https://cmr.earthdata.nasa.gov/search/collections.json'\n",
    "response = requests.get(cmr_collections_url, params=params)\n",
    "results = json.loads(response.content)\n",
    "\n",
    "# Find all instances of 'version_id' in metadata and print most recent version number\n",
    "\n",
    "versions = [el['version_id'] for el in results['feed']['entry']]\n",
    "latest_version = max(versions)\n",
    "print('The most recent version of ', short_name, ' is ', latest_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the most recent version of this data set, let's determine the number of granules available over our area and time of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input start date in yyyy-MM-dd format:  2018-11-10\n",
      "Input start time in HH:mm:ss format:  00:00:00\n",
      "Input end date in yyyy-MM-dd format:  2018-11-11\n",
      "Input end time in HH:mm:ss format:  00:00:00\n"
     ]
    }
   ],
   "source": [
    "#Input temporal range \n",
    "\n",
    "start_date = input('Input start date in yyyy-MM-dd format: ')\n",
    "start_time = input('Input start time in HH:mm:ss format: ')\n",
    "end_date = input('Input end date in yyyy-MM-dd format: ')\n",
    "end_time = input('Input end time in HH:mm:ss format: ')\n",
    "\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our area of interest, let's first choose from the following spatial region selections:\n",
    "#### 1) Bounding Box \n",
    "#### 2) Polygon coordinate pairs \n",
    "#### 3) Spatial file input, including Esri Shapefile or KML/KMZ. \n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your selected numbered option (1, 2, or 3)\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "aoi = str(input('''Enter your selected numbered option (1, 2, or 3)\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input lower left longitude in decimal degrees:  -120\n",
      "Input lower left latitude in decimal degrees:  70\n",
      "Input upper right longitude in decimal degrees:  -115\n",
      "Input upper right latitude in decimal degrees:  80\n"
     ]
    }
   ],
   "source": [
    "#prompt for spatial region information depending on input selection\n",
    "\n",
    "if aoi == '1': \n",
    "    #Input bounding box\n",
    "    LL_lon = input('Input lower left longitude in decimal degrees: ')\n",
    "    LL_lat = input('Input lower left latitude in decimal degrees: ')\n",
    "    UR_lon = input('Input upper right longitude in decimal degrees: ')\n",
    "    UR_lat = input('Input upper right latitude in decimal degrees: ')\n",
    "    \n",
    "    bounding_box = LL_lon + ',' + LL_lat + ',' + UR_lon + ',' + UR_lat\n",
    "    \n",
    "    #CMR parameters used for granule search\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'bounding_box': bounding_box,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1\n",
    "    }\n",
    "    \n",
    "elif aoi == '2':\n",
    "    print('Polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon.')\n",
    "    polygon = input('''Input polygon coordinates as comma separated values in longitude latitude order, i.e. lon1, lat1, lon2, lat2, lon3, lat3, and so on: \n",
    "    ''')\n",
    "    \n",
    "    #CMR parameters used for granule search\n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'bounding_box': '',\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1\n",
    "    }\n",
    "    \n",
    "elif aoi == '3':\n",
    "    #You may want a backup method in case the service goes down or gets bogged down at the tutorial (parse kml with xml library, or a library that parses kml like fiona). \n",
    "    \n",
    "    #POST shapefile or KML polygon to OGR for geojson conversion\n",
    "     \n",
    "    url = 'http://ogre.adc4gis.com/convert'\n",
    "    shapefile = str(os.getcwd()) + '/pine_island_glims/glims_polygons.kml'\n",
    "    files = {'upload': open(shapefile, 'rb')}\n",
    "    r = requests.post(url, files=files)\n",
    "    results = json.loads(r.content)\n",
    "    \n",
    "    #results is a dictionary representing a feature collection. List coordinates from the Polygon feature:\n",
    "    polygon_list = list(results['features'][0]['geometry']['coordinates'][0])\n",
    "    \n",
    "    \n",
    "    #Remove z value from polygon list\n",
    "    for i in range(len(polygon_list)):\n",
    "        del polygon_list[i][2]\n",
    "    \n",
    "    #Create shapely Polygon object for simplification and counter-clockwise ordering for CMR filtering\n",
    "    poly = Polygon(tuple(polygon_list))\n",
    "    \n",
    "    #simplify polygon\n",
    "    s = poly.simplify(0.05, preserve_topology=False)\n",
    "    \n",
    "    #Orient counter-clockwise\n",
    "    cc = orient(s, sign=1.0)\n",
    "    \n",
    "    #from polygon object back to dictionary\n",
    "    poly_dict = mapping(cc)\n",
    "\n",
    "    #Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "\n",
    "    c = list(poly_dict['coordinates'][0])\n",
    "    polygon = ','.join(map(str, list(sum(c, ()))))\n",
    "    \n",
    "    #Find bounds of polygon for CMR filtering\n",
    "    \n",
    "    #Create shapely polygon object\n",
    "    #poly = Polygon(tuple(polygon_list))\n",
    "    #bounds = poly.bounds\n",
    "    #minx, miny, maxx, maxy\n",
    "    \n",
    "    \n",
    "    #Flatten list, convert to tuple for CMR polygon parameter\n",
    "    #polygon = sum(polygon_list, [])\n",
    "    \n",
    "    #polygon = ','.join(map(str, (sum(polygon_list, [])))) \n",
    "    \n",
    "    #CMR parameters used for granule search\n",
    "    \n",
    "    params = {\n",
    "    'short_name': short_name,\n",
    "    'version': latest_version,\n",
    "    'polygon': polygon,\n",
    "    'temporal': temporal,\n",
    "    'page_size': 100,\n",
    "    'page_num': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the KML of Pine Island Glacier from the NSIDC [Global Land Ice Measurements from Space (GLIMS) database](http://www.glims.org/maps/glims). \n",
    "\n",
    "#### The Pine Island Glacier outline was downloaded from the following source: http://www.glims.org/maps/download.html?download_type=id&clause=G263560E76894S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-fe9db5946193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import geojson python dictionary as Geodataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#render figure in notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'features'"
     ]
    }
   ],
   "source": [
    "#import geojson python dictionary as Geodataframe\n",
    "gdf = gpd.GeoDataFrame.from_features(results['features'])\n",
    "\n",
    "#render figure in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "polygon_plot = gdf.plot(color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 granules of ATL06 version 001 over my area and time of interest.\n"
     ]
    }
   ],
   "source": [
    "# Query number of granules (paging over results)\n",
    "\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "\n",
    "granules = []\n",
    "while True:\n",
    "    response = requests.get(granule_search_url, params=params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    params['page_num'] += 1\n",
    "\n",
    "print('There are', len(granules), 'granules of', short_name, 'version', latest_version, 'over my area and time of interest.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now query the average size of those granules as well as the total volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average size of each granule is 31.84 MB and the total size of all 2 granules is 63.69 MB\n"
     ]
    }
   ],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "\n",
    "print(f'The average size of each granule is {mean(granule_sizes):.2f} MB and the total size of all {len(granules)} granules is {sum(granule_sizes):.2f} MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although subsetting, reformatting, or reprojecting can alter the size of the granules, this \"native\" granule size can still be used to guide us towards the best download method to pursue, which we will come back to later on in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the subsetting and reformatting services enabled for your data set of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NSIDC DAAC supports customization services on many of our NASA Earthdata mission collections. Reformatting and subsetting are available on all Level-2 and -3 ICESat-2 data sets. Let's discover the specific service options supported for this data set and select which of these services we want to request. \n",
    "\n",
    "### We will start by querying the service capability endpoint and gather service information that we will select in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query service capability URL \n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "\n",
    "# Create session to store cookie and pass credentials to capabilities url\n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "response = session.get(s.url,auth=(uid,pswd))\n",
    "\n",
    "root = ET.fromstring(response.content)\n",
    "\n",
    "#collect lists with each service option\n",
    "\n",
    "subagent = [subset_agent.attrib for subset_agent in root.iter('SubsetAgent')]\n",
    "\n",
    "# variable subsetting\n",
    "variables = [SubsetVariable.attrib for SubsetVariable in root.iter('SubsetVariable')]  \n",
    "variables_raw = [variables[i]['value'] for i in range(len(variables))]\n",
    "variables_join = [''.join(('/',v)) if v.startswith('/') == False else v for v in variables_raw] \n",
    "variable_vals = [v.replace(':', '/') for v in variables_join]\n",
    "\n",
    "# reformatting\n",
    "formats = [Format.attrib for Format in root.iter('Format')]\n",
    "format_vals = [formats[i]['value'] for i in range(len(formats))]\n",
    "format_vals.remove('')\n",
    "\n",
    "# reprojection only applicable on ICESat-2 L3B products, yet to be available. \n",
    "\n",
    "# reformatting options that support reprojection\n",
    "normalproj = [Projections.attrib for Projections in root.iter('Projections')]\n",
    "normalproj_vals = []\n",
    "normalproj_vals.append(normalproj[0]['normalProj'])\n",
    "format_proj = normalproj_vals[0].split(',')\n",
    "format_proj.remove('')\n",
    "format_proj.append('No reformatting')\n",
    "\n",
    "#reprojection options\n",
    "projections = [Projection.attrib for Projection in root.iter('Projection')]\n",
    "proj_vals = []\n",
    "for i in range(len(projections)):\n",
    "    if (projections[i]['value']) != 'NO_CHANGE' :\n",
    "        proj_vals.append(projections[i]['value'])\n",
    "        \n",
    "# reformatting options that do not support reprojection\n",
    "no_proj = [i for i in format_vals if i not in format_proj]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now select subsetting and reformatting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting by bounding box, based on the area of interest inputted above, is available.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to request this service? (y/n) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting by time, based on the temporal range inputted above, is available.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to request this service? (y/n) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These reformatting options are available: ['TABULAR_ASCII', 'NetCDF4-CF', 'Shapefile', 'NetCDF-3']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No reprojection options are supported with your requested format\n"
     ]
    }
   ],
   "source": [
    "#print service information depending on service availability and select service options\n",
    "    \n",
    "if len(subagent) < 1 :\n",
    "    print('No services exist for', short_name, 'version', latest_version)\n",
    "    agent = 'NO'\n",
    "    bbox = ''\n",
    "    timevar = ''\n",
    "    reformat = ''\n",
    "    projection = ''\n",
    "    projection_parameters = ''\n",
    "    coverage = ''\n",
    "    polygon = ''\n",
    "else:\n",
    "    agent = ''\n",
    "    subdict = subagent[0]\n",
    "    #icesat-2 spatial subsetting logic\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '1':\n",
    "        print('Subsetting by bounding box, based on the area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bounding_shape = ''\n",
    "        polygon = ''\n",
    "        if ss == 'y': \n",
    "            bbox = bounding_box\n",
    "        else: \n",
    "            bbox = ''\n",
    "    #if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi != '2':\n",
    "    #    print('Subsetting by polygon, based on area of interest inputted above, is available.')\n",
    "    #    ss = input('Would you like to request this service? (y/n)')\n",
    "    #    if ss == 'y':\n",
    "    #        #convert to dictionary for geojson input\n",
    "    #        ##poly_dict = mapping(cc)\n",
    "    #    else: \n",
    "    #        bounding_shape = ''\n",
    "    #        bbox = ''\n",
    "    \n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] == 'ICESAT2' and aoi == '3':\n",
    "        print('Subsetting by polygon, based on the original spatial file inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        bbox = ''\n",
    "        bounding_box = ''\n",
    "        bounding_shape = ''\n",
    "        if ss == 'y': upload = 'T'\n",
    "    if subdict['spatialSubsetting'] == 'true' and subdict['id'] != 'ICESAT2':\n",
    "        polygon = ''\n",
    "        print('Subsetting by bounding box, based on the area of interest inputted above, is available.')\n",
    "        ss = input('Would you like to request this service? (y/n)')\n",
    "        if ss == 'y': bbox = bounding_box\n",
    "        else: \n",
    "            bbox = ''\n",
    "            polygon = ''\n",
    "    if subdict['temporalSubsetting'] == 'true':\n",
    "        print('Subsetting by time, based on the temporal range inputted above, is available.')\n",
    "        ts = input('Would you like to request this service? (y/n)')\n",
    "        if ts == 'y': timevar = temporal \n",
    "        else: timevar = ''\n",
    "    else: timevar = ''\n",
    "    if len(format_vals) > 0 :\n",
    "        print('These reformatting options are available:', format_vals)\n",
    "        reformat = input('If you would like to reformat, copy and paste the reformatting option you would like (make sure to omit quotes, e.g. GeoTIFF), otherwise leave blank.')\n",
    "        # select reprojection options based on reformatting selection\n",
    "        if reformat in format_proj and len(proj_vals) > 0 : \n",
    "            print('These reprojection options are available with your requested format:', proj_vals)\n",
    "            projection = input('If you would like to reproject, copy and paste the reprojection option you would like (make sure to omit quotes, e.g. GEOGRAPHIC), otherwise leave blank.')\n",
    "            # Enter required parameters for UTM North and South\n",
    "            if projection == 'UTM NORTHERN HEMISPHERE' or projection == 'UTM SOUTHERN HEMISPHERE': \n",
    "                NZone = input('Please enter a UTM zone (1 to 60 for Northern Hemisphere; -60 to -1 for Southern Hemisphere):')\n",
    "                projection_parameters = str('NZone:' + NZone)\n",
    "            else: projection_parameters = ''\n",
    "        else: \n",
    "            print('No reprojection options are supported with your requested format')\n",
    "            projection = ''\n",
    "            projection_parameters = ''\n",
    "    else: \n",
    "        reformat = ''\n",
    "        projection = ''\n",
    "        projection_parameters = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because variable subsetting can include a long list of variables to choose from, we will decide on variable subsetting separately from the service options above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Variable subsetting is available. Would you like to subset a selection of variables? (y/n) n\n"
     ]
    }
   ],
   "source": [
    "# Select variable subsetting\n",
    "\n",
    "if len(variable_vals) > 0:\n",
    "        v = input('Variable subsetting is available. Would you like to subset a selection of variables? (y/n)')\n",
    "        if v == 'y':\n",
    "            print('The', short_name, 'variables to select from include:')\n",
    "            pprint.pprint(variable_vals)\n",
    "            coverage = input('If you would like to subset by variable, copy and paste the variables you would like separated by comma. Make sure to omit quotes but include all forward slashes: ')\n",
    "        else: coverage = ''\n",
    "\n",
    "#no services selected\n",
    "if reformat == '' and projection == '' and projection_parameters == '' and coverage == '' and timevar == '' and bbox == '' and aoi != '3':\n",
    "    agent = 'NO'\n",
    "else: agent = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request data from the NSIDC data access API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _What is an API? API stands for Application Programming Interface. You can think of it as a middle man between an application or end-use (in this case, us) and a data provider (in this case, the Common Metadata Repository and NSIDC). These APIs are essentially structured as a URL with a base plus individual key-value-pairs (KVPs) separated by ‘&’._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now set up our data download request. Recall that we queried the total number and volume of granules prior to applying customization services, so you can use these values to adjust the number of granules per request up to a limit of 100 granules. For now, let's select 10 granules to be processed in each zipped request. We will also set the request mode to asynchronous, which will allow concurrent requests to be queued and processed without the need for a continuous connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 1 total order(s) processed for our ATL06 request.\n"
     ]
    }
   ],
   "source": [
    "# Determine how many individual orders we will request based on the number of granules requested\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "\n",
    "#Set request mode\n",
    "request_mode = 'async'\n",
    "\n",
    "#Set NSIDC data access base URL\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request'\n",
    "\n",
    "print('There will be', page_num, 'total order(s) processed for our', short_name, 'request.')\n",
    "\n",
    "#Add print of all filter and customization parameters that will be applied to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's download the data directly to this notebook directory in a new Outputs folder. The progress of each order will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'short_name': 'ATL06', 'version': '001', 'temporal': '2018-11-10T00:00:00Z,2018-11-11T00:00:00Z', 'time': '', 'bounding_box': '-120,70,-115,80', 'polygon': '', 'bbox': '-120,70,-115,80', 'bounding_shape': '', 'format': '', 'projection': '', 'projection_parameters': '', 'Coverage': '', 'request_mode': 'async', 'page_size': 10, 'page_num': 1, 'agent': '', 'token': '67F8A83F-CB95-D836-45CF-30638D6DE8AD', 'email': 'amy.steiker@nsidc.org'}\n",
      "order ID 5000000317075\n",
      "Data request 1 is submitting...\n",
      "Initial request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is pending\n",
      "Status is not complete. trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. trying again.\n",
      "Retry request status is processing\n",
      "Status is not complete. trying again.\n",
      "Retry request status is complete_with_errors\n",
      "Request status is complete. Beginning download of zipped output.\n",
      "Data request 1 is complete.\n"
     ]
    }
   ],
   "source": [
    "# Create Outputs folder if folder does not already exist, request data service for each page number, and unzip outputs\n",
    "\n",
    "\n",
    "path = str(os.getcwd() + '/Outputs')\n",
    "\n",
    "if os.path.exists(path):\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        request_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'time': timevar, 'bounding_box': bounding_box, 'polygon': polygon, 'bbox': bbox, 'bounding_shape': bounding_shape, 'format': reformat, 'projection': projection, 'projection_parameters': projection_parameters, 'Coverage': coverage, 'request_mode': request_mode, 'page_size': page_size, 'page_num': page_val, 'agent': agent, 'token': token, 'email': email, }\n",
    "        print(request_params)\n",
    "        #Initial order response \n",
    "        data_session = requests.session()\n",
    "        data_s = data_session.get(base_url, params=request_params)\n",
    "        data_response = session.get(data_s.url,auth=(uid,pswd))\n",
    "        esir_root = ET.fromstring(data_response.content)\n",
    "        #Look up order ID\n",
    "        #orderID = esir_root[0][0].text\n",
    "        orderlist = []   \n",
    "        for order in esir_root.findall(\"./order/\"):\n",
    "            orderlist.append(order.text)\n",
    "        orderID = orderlist[0]\n",
    "        print('order ID' + ' ' + orderID)\n",
    "\n",
    "        #Create status URL\n",
    "        statusURL = base_url + '/' + orderID\n",
    "        #Find order status\n",
    "        request_session = requests.session()\n",
    "        request_s = request_session.get(statusURL)\n",
    "        request_response = session.get(request_s.url,auth=(uid,pswd))\n",
    "        request_root = ET.fromstring(request_response.content)\n",
    "        statuslist = []\n",
    "        for status in request_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Data request', page_val, 'is submitting...')\n",
    "        print('Initial request status is ' + status)\n",
    "\n",
    "        #Continue loop while request is still processing\n",
    "        while status == 'pending' or status == 'processing': \n",
    "            print('Status is not complete. Trying again.')\n",
    "            time.sleep(5)\n",
    "            loop_session = requests.session()\n",
    "            loop_s = loop_session.get(statusURL)\n",
    "            loop_response = session.get(loop_s.url,auth=(uid,pswd))\n",
    "            loop_root = ET.fromstring(loop_response.content)\n",
    "            #find status\n",
    "            statuslist = []\n",
    "            for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            print('Retry request status is' + ' ' + status)\n",
    "            if status == 'pending' or status == 'processing':\n",
    "                continue\n",
    "\n",
    "        print('Request status is complete. Beginning download of zipped output.')\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        zip_session = requests.session()\n",
    "        zip_s = zip_session.get(downloadURL)\n",
    "        zip_response = session.get(zip_s.url,auth=(uid,pswd))\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "else:\n",
    "    path = str(os.getcwd() + '/Outputs')\n",
    "    os.mkdir(path)\n",
    "    for i in range(page_num):\n",
    "        page_val = i + 1\n",
    "        request_params = {'short_name': short_name, 'version': latest_version, 'temporal': temporal, 'time': timevar, 'bounding_box': bounding_box, 'polygon': polygon, 'bbox': bbox, 'bounding_shape': bounding_shape, 'format': reformat, 'projection': projection, 'projection_parameters': projection_parameters, 'Coverage': coverage, 'request_mode': request_mode, 'page_size': page_size, 'page_num': page_val, 'agent': agent, 'token': token, 'email': email, }\n",
    "        \n",
    "        #Initial order response \n",
    "        data_session = requests.session()\n",
    "        data_s = data_session.get(base_url, params=request_params)\n",
    "        data_response = session.get(data_s.url,auth=(uid,pswd))\n",
    "        esir_root = ET.fromstring(data_response.content)\n",
    "\n",
    "        #Look up order ID\n",
    "        #orderID = esir_root[0][0].text\n",
    "        orderlist = []   \n",
    "        for order in esir_root.findall(\"./order/\"):\n",
    "            orderlist.append(order.text)\n",
    "        orderID = orderlist[0]\n",
    "        print('order ID' + ' ' + orderID)\n",
    "\n",
    "        #Create status URL\n",
    "        statusURL = base_url + '/' + orderID\n",
    "        #Find order status\n",
    "        request_session = requests.session()\n",
    "        request_s = request_session.get(statusURL)\n",
    "        request_response = session.get(request_s.url,auth=(uid,pswd))\n",
    "        request_root = ET.fromstring(request_response.content)\n",
    "        statuslist = []\n",
    "        for status in request_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Data request', page_val, 'is submitting...')\n",
    "        print('Initial request status is ' + status)\n",
    "\n",
    "        #Continue loop while request is still processing\n",
    "        while status == 'pending' or status == 'processing':\n",
    "            print('Status is not complete. Trying again.')\n",
    "            time.sleep(5)\n",
    "            loop_session = requests.session()\n",
    "            loop_s = loop_session.get(statusURL)\n",
    "            loop_response = session.get(loop_s.url,auth=(uid,pswd))\n",
    "            loop_root = ET.fromstring(loop_response.content)\n",
    "            #find status\n",
    "            statuslist = []\n",
    "            for status in loop_root.findall(\"./requestStatus/\"):\n",
    "                statuslist.append(status.text)\n",
    "            status = statuslist[0]\n",
    "            print('Retry request status is' + ' ' + status)\n",
    "            if status == 'pending' or status == 'processing':\n",
    "                continue\n",
    "\n",
    "        print('Request status is complete. Beginning download of zipped output.')\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        zip_session = requests.session()\n",
    "        zip_s = zip_session.get(downloadURL)\n",
    "        zip_response = session.get(zip_s.url,auth=(uid,pswd))\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(path)\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "\n",
    "r = requests.get(base_url, params=request_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we will clean up the Output folder by removing individual order folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up Outputs folder by removing individual granule folders \n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for file in files:\n",
    "        try:\n",
    "            shutil.move(os.path.join(root, file), path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        \n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_ATL06_20181110214700_06610103_001_01.h5',\n",
       " 'ATL06_20181110210500_06600111_001_01.iso.xml',\n",
       " 'ATL06_20181110214700_06610103_001_01.iso.xml',\n",
       " 'ATL06_20181110082300_06520110_001_01.h5',\n",
       " 'ATL06_20181110105733_06540105_001_01.h5',\n",
       " 'ATL06_20181110211043_06600112_001_01.iso.xml',\n",
       " 'ATL06_20181110083041_06520111_001_01.h5',\n",
       " 'ATL06_20181110211043_06600112_001_01.h5',\n",
       " 'ATL06_20181110210500_06600111_001_01.h5',\n",
       " 'ATL06_20181110214700_06610103_001_01.h5',\n",
       " 'ATL06_20181110105733_06540105_001_01.iso.xml',\n",
       " 'ATL06_20181110083041_06520111_001_01.iso.xml',\n",
       " 'ATL06_20181110082300_06520110_001_01.iso.xml']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List files\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To review, we have explored data availability and volume over a region and time of interest, discovered and selected data customization options, and downloaded data directly to our local machine. You are welcome to request different data sets, areas of interest, and/or customization services by re-running the notebook or starting again at the 'Select a data set of interest' step above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
